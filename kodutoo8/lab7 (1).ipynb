{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPCom-KmApV"
      },
      "source": [
        "# Basic Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PslnzB4Y0xPo"
      },
      "source": [
        "### Setting up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook setup**. Select the \"Edit\" menu, then \"Notebook settings\". Choose \"GPU\" as the hardware accelerator. Check that \"Omit code cell output...\" is unchecked (so that the output is saved).\n",
        "\n"
      ],
      "metadata": {
        "id": "o5O0p0HC03C5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7KBpffWzlxH"
      },
      "source": [
        "### Pytorch, dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAve6DCL4JH4"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "BIRVWB6F-QM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oewp-wYg31t9"
      },
      "source": [
        "### Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data.data.size())\n",
        "print(training_data.targets.size())\n",
        "print(test_data.data.size())\n",
        "print(test_data.targets.size())"
      ],
      "metadata": {
        "id": "W5hfcRDNQtNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data of one image"
      ],
      "metadata": {
        "id": "LWeKN_HbRbR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_image, first_label = training_data[0]\n",
        "first_image = first_image.numpy().squeeze()\n",
        "print(f\"Shape of first image: {first_image.shape}\")\n",
        "print(first_image)"
      ],
      "metadata": {
        "id": "qK5hL0UsRim8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this is what the image looks like (a shoe?)"
      ],
      "metadata": {
        "id": "6hIlZOVySIxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(first_image, cmap='gray')"
      ],
      "metadata": {
        "id": "sCZoxyQcSL3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The label of the image (see the list of labels at https://github.com/zalandoresearch/fashion-mnist)"
      ],
      "metadata": {
        "id": "OiejkO3OTh0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Label of first image: {first_label}\")"
      ],
      "metadata": {
        "id": "IGyqRUNMTuU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put you code to create, train and test the model below"
      ],
      "metadata": {
        "id": "FRLdpO4AWXYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup\n",
        "import torch\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# 2. Imports\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 3. Load and preprocess Fashion-MNIST dataset\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n",
        "\n",
        "# 4. Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cnn_stack(x)\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# 5. Training function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  {current:>5d}/{size:>5d}\")\n",
        "\n",
        "# 6. Train the model\n",
        "epochs = 15\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Done!\")\n",
        "\n",
        "# 7. Evaluate on the test set\n",
        "model.eval()\n",
        "\n",
        "all_outputs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        all_outputs.append(outputs)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "all_outputs = torch.cat(all_outputs, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "# 8. View results for the first image\n",
        "print(all_outputs[0])\n",
        "print(torch.sum(all_outputs[0]))\n",
        "\n",
        "first_image, _ = test_data[0]\n",
        "first_image = first_image.numpy().squeeze()\n",
        "plt.imshow(first_image, cmap='gray')\n",
        "plt.title(\"First test image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"Predicted class index:\", torch.argmax(all_outputs[0]))\n",
        "print(\"Actual class index:\", all_labels[0])\n",
        "\n",
        "# 9. Accuracy calculation\n",
        "all_preds = torch.argmax(all_outputs, dim=1)\n",
        "correct = torch.sum(all_preds.cpu() == all_labels)\n",
        "print(\"Correct classes: {} of {} (accuracy: {:.2f}%)\".format(correct, 10000, 100.0 * correct.item() / 10000))\n",
        "\n",
        "# 10. Theoretical random baseline accuracy\n",
        "print(\"Random guessing accuracy would be about 10% (1 out of 10 classes)\")\n"
      ],
      "metadata": {
        "id": "NW3_QC9d59d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33222cce-0ee4-457d-c778-2d8621a8ae69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.5MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 198kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.72MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.295951     64/60000\n",
            "loss: 0.725571   6464/60000\n",
            "loss: 0.360848  12864/60000\n",
            "loss: 0.544381  19264/60000\n",
            "loss: 0.479648  25664/60000\n",
            "loss: 0.476783  32064/60000\n",
            "loss: 0.381665  38464/60000\n",
            "loss: 0.582635  44864/60000\n",
            "loss: 0.467249  51264/60000\n",
            "loss: 0.410710  57664/60000\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.288582     64/60000\n",
            "loss: 0.348010   6464/60000\n",
            "loss: 0.217728  12864/60000\n",
            "loss: 0.433184  19264/60000\n",
            "loss: 0.378354  25664/60000\n",
            "loss: 0.342011  32064/60000\n",
            "loss: 0.275297  38464/60000\n",
            "loss: 0.488675  44864/60000\n",
            "loss: 0.376146  51264/60000\n",
            "loss: 0.293395  57664/60000\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.255477     64/60000\n",
            "loss: 0.312786   6464/60000\n",
            "loss: 0.182369  12864/60000\n",
            "loss: 0.335345  19264/60000\n",
            "loss: 0.338148  25664/60000\n",
            "loss: 0.321272  32064/60000\n",
            "loss: 0.248568  38464/60000\n",
            "loss: 0.387388  44864/60000\n",
            "loss: 0.275287  51264/60000\n",
            "loss: 0.214748  57664/60000\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.225672     64/60000\n",
            "loss: 0.246115   6464/60000\n",
            "loss: 0.154877  12864/60000\n",
            "loss: 0.269558  19264/60000\n",
            "loss: 0.325129  25664/60000\n",
            "loss: 0.297619  32064/60000\n",
            "loss: 0.208518  38464/60000\n",
            "loss: 0.336040  44864/60000\n",
            "loss: 0.219514  51264/60000\n",
            "loss: 0.188031  57664/60000\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.197989     64/60000\n",
            "loss: 0.184101   6464/60000\n",
            "loss: 0.123564  12864/60000\n",
            "loss: 0.210644  19264/60000\n",
            "loss: 0.341201  25664/60000\n",
            "loss: 0.275883  32064/60000\n",
            "loss: 0.192556  38464/60000\n",
            "loss: 0.290157  44864/60000\n",
            "loss: 0.191368  51264/60000\n",
            "loss: 0.166850  57664/60000\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.156503     64/60000\n",
            "loss: 0.170144   6464/60000\n",
            "loss: 0.118028  12864/60000\n",
            "loss: 0.180513  19264/60000\n",
            "loss: 0.312118  25664/60000\n",
            "loss: 0.235232  32064/60000\n",
            "loss: 0.183242  38464/60000\n",
            "loss: 0.226009  44864/60000\n",
            "loss: 0.154209  51264/60000\n",
            "loss: 0.154636  57664/60000\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.134545     64/60000\n",
            "loss: 0.147857   6464/60000\n",
            "loss: 0.086814  12864/60000\n",
            "loss: 0.163476  19264/60000\n",
            "loss: 0.256666  25664/60000\n",
            "loss: 0.224193  32064/60000\n",
            "loss: 0.167813  38464/60000\n",
            "loss: 0.192518  44864/60000\n",
            "loss: 0.106434  51264/60000\n",
            "loss: 0.156274  57664/60000\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.110175     64/60000\n",
            "loss: 0.135100   6464/60000\n",
            "loss: 0.076977  12864/60000\n",
            "loss: 0.132235  19264/60000\n",
            "loss: 0.236390  25664/60000\n",
            "loss: 0.214427  32064/60000\n",
            "loss: 0.144874  38464/60000\n",
            "loss: 0.156924  44864/60000\n",
            "loss: 0.092290  51264/60000\n",
            "loss: 0.126059  57664/60000\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.111642     64/60000\n",
            "loss: 0.128291   6464/60000\n",
            "loss: 0.069677  12864/60000\n",
            "loss: 0.115667  19264/60000\n",
            "loss: 0.202203  25664/60000\n",
            "loss: 0.213416  32064/60000\n",
            "loss: 0.125622  38464/60000\n",
            "loss: 0.151656  44864/60000\n",
            "loss: 0.084038  51264/60000\n",
            "loss: 0.091574  57664/60000\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.101766     64/60000\n",
            "loss: 0.117536   6464/60000\n",
            "loss: 0.092583  12864/60000\n",
            "loss: 0.054245  19264/60000\n",
            "loss: 0.183564  25664/60000\n",
            "loss: 0.173044  32064/60000\n",
            "loss: 0.133046  38464/60000\n",
            "loss: 0.128529  44864/60000\n",
            "loss: 0.076693  51264/60000\n",
            "loss: 0.097892  57664/60000\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.123128     64/60000\n",
            "loss: 0.143842   6464/60000\n",
            "loss: 0.068684  12864/60000\n",
            "loss: 0.062331  19264/60000\n",
            "loss: 0.128467  25664/60000\n",
            "loss: 0.249238  32064/60000\n",
            "loss: 0.090602  38464/60000\n",
            "loss: 0.091867  44864/60000\n",
            "loss: 0.065791  51264/60000\n",
            "loss: 0.054296  57664/60000\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.111508     64/60000\n",
            "loss: 0.091779   6464/60000\n",
            "loss: 0.052627  12864/60000\n",
            "loss: 0.066394  19264/60000\n",
            "loss: 0.193166  25664/60000\n",
            "loss: 0.194998  32064/60000\n",
            "loss: 0.068806  38464/60000\n",
            "loss: 0.057313  44864/60000\n",
            "loss: 0.076067  51264/60000\n",
            "loss: 0.064228  57664/60000\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.111535     64/60000\n",
            "loss: 0.049366   6464/60000\n",
            "loss: 0.045146  12864/60000\n",
            "loss: 0.085409  19264/60000\n",
            "loss: 0.104125  25664/60000\n",
            "loss: 0.107840  32064/60000\n",
            "loss: 0.045870  38464/60000\n",
            "loss: 0.058801  44864/60000\n",
            "loss: 0.089629  51264/60000\n",
            "loss: 0.094166  57664/60000\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.098510     64/60000\n",
            "loss: 0.050194   6464/60000\n",
            "loss: 0.057036  12864/60000\n",
            "loss: 0.054702  19264/60000\n",
            "loss: 0.113773  25664/60000\n",
            "loss: 0.125004  32064/60000\n",
            "loss: 0.062672  38464/60000\n",
            "loss: 0.082345  44864/60000\n",
            "loss: 0.049873  51264/60000\n",
            "loss: 0.114910  57664/60000\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.083068     64/60000\n",
            "loss: 0.085901   6464/60000\n",
            "loss: 0.023570  12864/60000\n",
            "loss: 0.078600  19264/60000\n",
            "loss: 0.152988  25664/60000\n",
            "loss: 0.070424  32064/60000\n",
            "loss: 0.058555  38464/60000\n",
            "loss: 0.049349  44864/60000\n",
            "loss: 0.046390  51264/60000\n",
            "loss: 0.100469  57664/60000\n",
            "Done!\n",
            "tensor([-17.1416, -12.9449, -12.6538, -19.9085, -21.7874, -10.2650, -17.2085,\n",
            "          0.7133, -10.6509,  16.8671], device='cuda:0')\n",
            "tensor(-104.9802, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: tensor(9, device='cuda:0')\n",
            "Actual class index: tensor(9)\n",
            "Correct classes: 9017 of 10000 (accuracy: 90.17%)\n",
            "Random guessing accuracy would be about 10% (1 out of 10 classes)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}